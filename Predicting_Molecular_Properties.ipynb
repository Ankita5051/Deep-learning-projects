{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankita5051/Deep-learning-projects/blob/main/Predicting_Molecular_Properties.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LwMR-XgMkxo6",
        "outputId": "611cc197-c651-442d-d6fb-d8a0acf1f894"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "664bada3",
        "outputId": "3a381f8c-6ca2-4ee8-f040-c500beb3e5c3"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import MessagePassing,global_mean_pool\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch_geometric.utils import softmax\n",
        "from math import sqrt\n",
        "import torch.optim as optim\n",
        "from torch_geometric.loader import DataLoader\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "6Lb3aSdblAWk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QM9 Dataset:\n",
        "\n",
        "The QM9 dataset is a widely used benchmark dataset in the field of graph neural networks (GNNs) and molecular property prediction. It contains about 134,000 small organic molecules with up to 9 heavy atoms (C, O, N, F). Each molecule is represented as a graph, where atoms are nodes and bonds are edges.\n",
        "\n",
        "Key features of QM9:\n",
        "- Number of graphs: ~134,000\n",
        "- Node features: Atom properties (e.g., atomic number, charge)\n",
        "- Edge features: Bond properties (e.g., bond type)\n",
        "- Graph labels: Various molecular properties (e.g., energy, dipole moment)\n",
        "\tAvailable target properties:\n",
        "0: mu, 1: alpha, 2: homo, 3: lumo, 4: gap, 5: r2, 6: zpve, 7: U0, 8: U, 9: H, 10: G, 11:Cv, 12: omega1\n",
        "\n",
        "Link: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.QM9.html\n",
        "\n",
        "The dataset is used for regression tasks, predicting molecular properties from graph structures.\n",
        "\n",
        "**Use Case:**\n",
        "We are going to use 1000 graphs for training, 100 graphs for validation and 100 graphs for test.\n",
        "We will use ‘0: mu (dipole moment)’, first property, as a label for the regression task.\n",
        "You will get the Data-Loaded in the code notebook.\n",
        "This is the Regression task so you have to take one label for every graph.\n"
      ],
      "metadata": {
        "id": "P5_hhtbZ2Rmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the QM9 dataset\n",
        "dataset = QM9(root='data/QM9')\n",
        "print(dataset.num_node_features)\n",
        "# Access a single graph in the dataset\n",
        "data = dataset[0]\n",
        "\n",
        "# Print general information about the dataset\n",
        "print(f\"Number of graphs in the dataset: {len(dataset)}\")\n",
        "print(f\"Number of features per node: {data.x.shape[1]}\")\n",
        "print(f\"Number of edge features: {data.edge_attr.shape[1]}\")\n",
        "print(f\"Number of nodes in the first graph: {data.num_nodes}\")\n",
        "print(f\"Number of edges in the first graph: {data.num_edges}\")\n",
        "print(f\"Number of edge features in the first graph: {data.edge_attr.size()}\")\n",
        "print(\"graph\")\n",
        "print(data)\n",
        "# Investigate the node features\n",
        "print(\"\\nNode features:\")\n",
        "print(data.x)\n",
        "\n",
        "# Investigate the edge features\n",
        "print(\"\\nEdge features:\")\n",
        "print(data.edge_attr)\n",
        "\n",
        "# Investigate the adjacency list (edges)\n",
        "print(\"\\nEdges (connectivity):\")\n",
        "print(data.edge_index)\n",
        "\n",
        "# Investigate the target properties (e.g., energy, dipole moment)\n",
        "print(\"\\nTarget properties:\")\n",
        "print(data.y)"
      ],
      "metadata": {
        "id": "C42JNs1vnlWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53cc52f8-6c1d-4e73-a98b-08c2dc2d9a90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "Number of graphs in the dataset: 130831\n",
            "Number of features per node: 11\n",
            "Number of edge features: 4\n",
            "Number of nodes in the first graph: 5\n",
            "Number of edges in the first graph: 8\n",
            "Number of edge features in the first graph: torch.Size([8, 4])\n",
            "graph\n",
            "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])\n",
            "\n",
            "Node features:\n",
            "tensor([[0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4.],\n",
            "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
            "\n",
            "Edge features:\n",
            "tensor([[1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.]])\n",
            "\n",
            "Edges (connectivity):\n",
            "tensor([[0, 0, 0, 0, 1, 2, 3, 4],\n",
            "        [1, 2, 3, 4, 0, 0, 0, 0]])\n",
            "\n",
            "Target properties:\n",
            "tensor([[    0.0000,    13.2100,   -10.5499,     3.1865,    13.7363,    35.3641,\n",
            "             1.2177, -1101.4878, -1101.4098, -1101.3840, -1102.0229,     6.4690,\n",
            "           -17.1722,   -17.2868,   -17.3897,   -16.1519,   157.7118,   157.7100,\n",
            "           157.7070]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing dataset for for the further process:"
      ],
      "metadata": {
        "id": "MoZ1NBW82cXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dataset:\n",
        "    data.y = data.y[:, 0:1]  # Only dipole moment (mu)\n",
        "\n",
        "# Normalize the target\n",
        "y_all = torch.cat([data.y for data in dataset])\n",
        "mean_y, std_y = y_all.mean(), y_all.std()\n",
        "for data in dataset:\n",
        "    data.y = (data.y - mean_y) / std_y\n",
        "\n",
        "# Train/val/test split\n",
        "train_dataset = dataset[:1000]\n",
        "val_dataset = dataset[1000:1100]\n",
        "test_dataset = dataset[1100:1200]\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "_C9HIJsGm_E9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the library implementation of following shallow embedding methods to generate the\n",
        " node embeddings and then compute the graph features by averaging all the node features.\n",
        " * DeepWalk (embedding_dimensions= 64, walk_length=10, num_walks=50)\n",
        " * Node2Vec(embedding_dimensions= 64, walk_length=10, num_walks=50, p=1,\n",
        " q=0.5)\n",
        " Now, implement a custom Deep Neural Network for the regression task. [Every graph\n",
        " has one embedding and corresponding label to be predicted]\n",
        "# Report the following:\n",
        " * RootMeanSquare Error (RMSE) Metric for each of the methods in the test set."
      ],
      "metadata": {
        "id": "jMgcuW_-2h52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class CustomDeepWalk:\n",
        "    def __init__(self, graph, dimensions=64, walk_length=10, num_walks=50, seed=42):\n",
        "        self.graph = graph\n",
        "        self.dimensions = dimensions\n",
        "        self.walk_length = walk_length\n",
        "        self.num_walks = num_walks\n",
        "        self.walks = []\n",
        "        self.seed = seed\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "    def simulate_random_walks(self):\n",
        "        nodes = list(self.graph.nodes())\n",
        "        for _ in range(self.num_walks):\n",
        "            np.random.shuffle(nodes)  # Shuffle node order for diverse walks\n",
        "            for node in nodes:\n",
        "                self.walks.append(self._single_walk(node))\n",
        "\n",
        "    def _single_walk(self, start_node):\n",
        "        walk = [start_node]\n",
        "        while len(walk) < self.walk_length:\n",
        "            current_node = walk[-1]\n",
        "            neighbors = list(self.graph.neighbors(current_node))\n",
        "            if len(neighbors) == 0:\n",
        "                break  # Prevent walk getting stuck\n",
        "            walk.append(np.random.choice(neighbors))\n",
        "        return walk\n",
        "\n",
        "    def train_model(self):\n",
        "        self.simulate_random_walks()\n",
        "        # Use sg=1 for Skip-Gram, workers=4 for speed, and epochs>1 for stability\n",
        "        self.model = Word2Vec(\n",
        "            sentences=self.walks,\n",
        "            vector_size=self.dimensions,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            workers=4,\n",
        "            sg=1,\n",
        "            epochs=10,\n",
        "            seed=self.seed\n",
        "        )\n",
        "\n",
        "    def extract_embeddings(self):\n",
        "        # Normalize embeddings for stability\n",
        "        embeddings = {}\n",
        "        for node in self.graph.nodes():\n",
        "            vec = self.model.wv[node]\n",
        "            norm_vec = vec / (np.linalg.norm(vec) + 1e-10)  # Avoid divide by zero\n",
        "            embeddings[node] = norm_vec\n",
        "        return embeddings\n"
      ],
      "metadata": {
        "id": "pfUuOpHroCT9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GQ18Kd5F3uKe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class CustomNode2Vec:\n",
        "    def __init__(self, graph, dimensions=64, walk_length=10, num_walks=50, p=1.0, q=1.0, seed=42):\n",
        "        self.graph = graph\n",
        "        self.dimensions = dimensions\n",
        "        self.walk_length = walk_length\n",
        "        self.num_walks = num_walks\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.walks = []\n",
        "        self.seed = seed\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "    def simulate_random_walks(self):\n",
        "        nodes = list(self.graph.nodes())\n",
        "        for _ in range(self.num_walks):\n",
        "            np.random.shuffle(nodes)  # Shuffle nodes for walk diversity\n",
        "            for node in nodes:\n",
        "                self.walks.append(self.node2vec_walk(walk_length=self.walk_length, start_node=node))\n",
        "\n",
        "    def node2vec_walk(self, walk_length, start_node):\n",
        "        walk = [start_node]\n",
        "        while len(walk) < walk_length:\n",
        "            current = walk[-1]\n",
        "            neighbors = list(self.graph.neighbors(current))\n",
        "            if len(neighbors) == 0:\n",
        "                break\n",
        "            if len(walk) == 1:\n",
        "                walk.append(np.random.choice(neighbors))\n",
        "            else:\n",
        "                prev = walk[-2]\n",
        "                probs = []\n",
        "                for neighbor in neighbors:\n",
        "                    if neighbor == prev:\n",
        "                        prob = 1.0 / self.p\n",
        "                    elif self.graph.has_edge(neighbor, prev):\n",
        "                        prob = 1.0\n",
        "                    else:\n",
        "                        prob = 1.0 / self.q\n",
        "                    probs.append(prob)\n",
        "                probs = np.array(probs)\n",
        "                probs /= probs.sum()\n",
        "                walk.append(np.random.choice(neighbors, p=probs))\n",
        "        return walk\n",
        "\n",
        "    def train_model(self):\n",
        "        self.simulate_random_walks()\n",
        "        self.model = Word2Vec(\n",
        "            sentences=self.walks,\n",
        "            vector_size=self.dimensions,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            workers=4,\n",
        "            sg=1,\n",
        "            epochs=10,\n",
        "            seed=self.seed\n",
        "        )\n",
        "\n",
        "    def extract_embeddings(self):\n",
        "        embeddings = {}\n",
        "        for node in self.graph.nodes():\n",
        "            vec = self.model.wv[node]\n",
        "            norm_vec = vec / (np.linalg.norm(vec) + 1e-10)\n",
        "            embeddings[node] = norm_vec\n",
        "        return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_deepwalk_embeddings(data, dimensions=64, walk_length=10, num_walks=50, seed=42):\n",
        "    \"\"\"\n",
        "    Generate node embeddings for a single graph using improved DeepWalk.\n",
        "    \"\"\"\n",
        "    # Create the networkx graph\n",
        "    G = nx.Graph()\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        G.add_edge(edge_index[0, i], edge_index[1, i])\n",
        "\n",
        "    # If the graph is disconnected, keep only the largest connected component for embedding\n",
        "    if not nx.is_connected(G):\n",
        "        largest_cc = max(nx.connected_components(G), key=len)\n",
        "        G = G.subgraph(largest_cc).copy()\n",
        "\n",
        "    deepwalk = CustomDeepWalk(\n",
        "        G,\n",
        "        dimensions=dimensions,\n",
        "        walk_length=walk_length,\n",
        "        num_walks=num_walks,\n",
        "        seed=seed\n",
        "    )\n",
        "    deepwalk.train_model()\n",
        "    return deepwalk.extract_embeddings()\n",
        "\n",
        "def get_node2vec_embeddings(data, dimensions=64, walk_length=10, num_walks=50, p=1, q=1, seed=42):\n",
        "    \"\"\"\n",
        "    Generate node embeddings for a single graph using improved Node2Vec.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        G.add_edge(edge_index[0, i], edge_index[1, i])\n",
        "\n",
        "    # Handle disconnected components\n",
        "    if not nx.is_connected(G):\n",
        "        largest_cc = max(nx.connected_components(G), key=len)\n",
        "        G = G.subgraph(largest_cc).copy()\n",
        "\n",
        "    node2vec = CustomNode2Vec(\n",
        "        G,\n",
        "        dimensions=dimensions,\n",
        "        walk_length=walk_length,\n",
        "        num_walks=num_walks,\n",
        "        p=p,\n",
        "        q=q,\n",
        "        seed=seed\n",
        "    )\n",
        "    node2vec.train_model()\n",
        "    return node2vec.extract_embeddings()\n"
      ],
      "metadata": {
        "id": "T9pgzh3QpXem"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def calculate_graph_features(node_embeddings, dataset, embedding_dim=64):\n",
        "    \"\"\"\n",
        "    Computes graph-level features by averaging node embeddings for each graph in the dataset.\n",
        "\n",
        "    Args:\n",
        "        node_embeddings (dict): Mapping from node identifiers to their embeddings (numpy arrays or tensors).\n",
        "        dataset (torch_geometric.data.Dataset): Dataset containing graphs.\n",
        "        embedding_dim (int): Dimension of the node embeddings.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of shape (num_graphs, embedding_dim) containing graph-level features.\n",
        "    \"\"\"\n",
        "    graph_features = []\n",
        "    for i in range(len(dataset)):\n",
        "        edge_index = dataset[i].edge_index.cpu().numpy()\n",
        "        unique_nodes = np.unique(edge_index)\n",
        "\n",
        "        node_features = []\n",
        "        for node in unique_nodes:\n",
        "            if node in node_embeddings:\n",
        "                emb = node_embeddings[node]\n",
        "                if isinstance(emb, torch.Tensor):\n",
        "                    node_features.append(emb)\n",
        "                else:\n",
        "                    # Convert numpy array to torch tensor\n",
        "                    node_features.append(torch.tensor(emb, dtype=torch.float32))\n",
        "        if node_features:\n",
        "            graph_feature = torch.stack(node_features).mean(dim=0)\n",
        "        else:\n",
        "            graph_feature = torch.zeros((embedding_dim,), dtype=torch.float32)\n",
        "        graph_features.append(graph_feature)\n",
        "\n",
        "    return torch.stack(graph_features)\n",
        "# For each graph in the dataset, build local node embeddings and mean-pool them for a graph feature\n",
        "def get_graph_feature_with_embedding(dataset, embedder_fn, **kwargs):\n",
        "    features = []\n",
        "    for i in range(len(dataset)):\n",
        "        node_emb = embedder_fn(dataset[i], **kwargs)\n",
        "        edge_index = dataset[i].edge_index.cpu().numpy()\n",
        "        unique_nodes = np.unique(edge_index)\n",
        "        node_vectors = []\n",
        "        for node in unique_nodes:\n",
        "            if node in node_emb:\n",
        "                node_vectors.append(torch.tensor(node_emb[node], dtype=torch.float32))\n",
        "        if node_vectors:\n",
        "            graph_feature = torch.stack(node_vectors).mean(0)\n",
        "        else:\n",
        "            graph_feature = torch.zeros(kwargs.get('dimensions', 64), dtype=torch.float32)\n",
        "        features.append(graph_feature)\n",
        "    return torch.stack(features)\n"
      ],
      "metadata": {
        "id": "FuqPiAHKp6Bf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SimpleRegressionNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=19):\n",
        "        super(SimpleRegressionNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)  # Predict 19 outputs by default\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "def train_neural_model(model, train_loader, num_epochs=100, learning_rate=0.001, patience=50, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_model_wts = None\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "        for features, targets in loop:\n",
        "            features, targets = features.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(features)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * features.size(0)\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        if(epoch +1 )%10==0:\n",
        "          print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = model.state_dict()\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "                break\n",
        "\n",
        "    if best_model_wts is not None:\n",
        "        model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def compute_rmse(model, test_loader, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    loss_fn = nn.MSELoss(reduction='sum')\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in test_loader:\n",
        "            features, targets = features.to(device), targets.to(device)\n",
        "            predictions = model(features)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "            total_loss += loss.item()\n",
        "            total_samples += features.size(0) * targets.size(1)\n",
        "    rmse = np.sqrt(total_loss / total_samples)\n",
        "    return rmse\n"
      ],
      "metadata": {
        "id": "-IEb6n38qO-F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def create_dataloader(features, targets, batch_size=32, shuffle=True, num_workers=4, pin_memory=True):\n",
        "    \"\"\"\n",
        "    Wraps features and targets in TensorDataset and returns a DataLoader with recommended settings.\n",
        "    \"\"\"\n",
        "    # Convert to tensors if not already\n",
        "    if not torch.is_tensor(features):\n",
        "        features = torch.tensor(features, dtype=torch.float32)\n",
        "    if not torch.is_tensor(targets):\n",
        "       # targets = torch.tensor(targets, dtype=torch.float32)\n",
        "        targets = torch.stack([data.y[0].unsqueeze(0) for data in dataset[:1200]]).float()\n",
        "    dataset = TensorDataset(features, targets)\n",
        "    return DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "        num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Yy0Vv1hTrpF0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_node_embeddings_n2v = {}\n",
        "all_node_embeddings_dw = {}\n",
        "for i, graph in enumerate(dataset[:1200]):\n",
        "    emb = get_node2vec_embeddings(graph, dimensions=64, walk_length=10, num_walks=50, p=1, q=0.5, seed=42)\n",
        "    emb_dw = get_deepwalk_embeddings(graph, dimensions=64, walk_length=10, num_walks=50, seed=42)\n",
        "    all_node_embeddings_n2v.update({(i, node): vec for node, vec in emb.items()})\n",
        "    all_node_embeddings_dw.update({(i, node): vec for node, vec in emb_dw.items()})\n"
      ],
      "metadata": {
        "id": "sfPCS7xIqZgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58997a45-eb64-48a6-fbbe-863b67dfe705"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/200:   0%|          | 0/32 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/200, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/200, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/200, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/200, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/200, Loss: nan\n",
            "Early stopping triggered after 50 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/200, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/200, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/200, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/200, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/200, Loss: nan\n",
            "Early stopping triggered after 50 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepWalk Model RMSE: nan\n",
            "Node2Vec Model RMSE: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Node2Vec embeddings and graph features\n",
        "graph_features_n2v = calculate_graph_features(all_node_embeddings_n2v, dataset, embedding_dim=64)\n",
        "\n",
        "# Generate deepwalk embeddings and graph features\n",
        "graph_features_dw = calculate_graph_features(all_node_embeddings_dw, dataset, embedding_dim=64)\n",
        "eps = 1e-8  # small number to prevent division by zero\n",
        "graph_features_n2v = torch.stack([f / (f.norm() + eps) for f in graph_features_n2v])\n",
        "graph_features_dw = torch.stack([f / (f.norm() + eps) for f in graph_features_dw])\n",
        "\n",
        "\n",
        "# Extract and prepare normalized target values (dipole moment scaled)\n",
        "targets = torch.stack([data.y.squeeze() for data in dataset[:1200]]).float()  # Assuming y normalized already\n",
        "targets_mean = targets.mean()\n",
        "targets_std = targets.std() + 1e-8  # prevent division by zero\n",
        "targets = (targets - targets_mean) / targets_std\n",
        "# Create dataloaders for training, validation and testing sets\n",
        "train_loader_n2v = create_dataloader(graph_features_n2v[:1000], targets[:1000], batch_size=32)\n",
        "val_loader_n2v = create_dataloader(graph_features_n2v[1000:1100], targets[1000:1100], batch_size=32, shuffle=False)\n",
        "test_loader_n2v = create_dataloader(graph_features_n2v[1100:1200], targets[1100:1200], batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_dw = create_dataloader(graph_features_dw[:1000], targets[:1000], batch_size=32)\n",
        "val_loader_dw = create_dataloader(graph_features_dw[1000:1100], targets[1000:1100], batch_size=32, shuffle=False)\n",
        "test_loader_dw = create_dataloader(graph_features_dw[1100:1200], targets[1100:1200], batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize and train the regression models using improved training function and early stopping\n",
        "regression_model_dw = SimpleRegressionNN(input_dim=64)\n",
        "regression_model_dw = train_neural_model(regression_model_dw, train_loader_dw, num_epochs=200, learning_rate=0.002, patience=50)\n",
        "\n",
        "regression_model_n2v = SimpleRegressionNN(input_dim=64)\n",
        "regression_model_n2v = train_neural_model(regression_model_n2v, train_loader_n2v, num_epochs=200, learning_rate=0.002, patience=50)\n",
        "\n",
        "# Evaluate the trained models on test sets\n",
        "rmse_dw = compute_rmse(regression_model_dw, test_loader_dw)\n",
        "rmse_n2v = compute_rmse(regression_model_n2v, test_loader_n2v)\n",
        "\n",
        "print(f\"DeepWalk Model RMSE: {rmse_dw:.4f}\")\n",
        "print(f\"Node2Vec Model RMSE: {rmse_n2v:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU_xawni_XJL",
        "outputId": "aa39b77f-ed98-4a7a-edfa-5e98a2977137"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/200, Loss: 0.736347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/200, Loss: 0.736717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/200, Loss: 0.736248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/200, Loss: 0.736705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/200, Loss: 0.736699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60/200, Loss: 0.736345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70/200, Loss: 0.736447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80/200, Loss: 0.736187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90/200, Loss: 0.736515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/200, Loss: 0.736218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 110/200, Loss: 0.736717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 120/200, Loss: 0.736408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 130/200, Loss: 0.736401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 140/200, Loss: 0.737180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 150/200, Loss: 0.736237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 160/200, Loss: 0.736423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping triggered after 164 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/200, Loss: 0.740454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/200, Loss: 0.736365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/200, Loss: 0.736300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/200, Loss: 0.736662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/200, Loss: 0.736376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60/200, Loss: 0.736724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70/200, Loss: 0.736273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80/200, Loss: 0.736719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90/200, Loss: 0.736412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/200, Loss: 0.736552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 110/200, Loss: 0.736518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 120/200, Loss: 0.736454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 130/200, Loss: 0.736460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 140/200, Loss: 0.736197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 150/200, Loss: 0.737023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 160/200, Loss: 0.736249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 170/200, Loss: 0.736250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 180/200, Loss: 0.736300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 190/200, Loss: 0.736348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 200/200, Loss: 0.736111\n",
            "DeepWalk Model RMSE: 0.1427\n",
            "Node2Vec Model RMSE: 0.1295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2\n",
        " * Implement a Graph Convolutional Network (GCN) using the original node\n",
        " features.\n",
        " * Youcantry out various aggregators like sum, mean etc to get graph\n",
        " features at the end.\n",
        " * Showtheeffect of GCN layers into the learning [use upto 4 GCN layers].\n",
        " * Perform Regression on the test set and report RMSE"
      ],
      "metadata": {
        "id": "tin3yGGdIHUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomGCNLayer(MessagePassing):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CustomGCNLayer, self).__init__(aggr='add')\n",
        "        self.linear_transform = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, node_features, edge_indices):\n",
        "\n",
        "        edge_indices, _ = add_self_loops(edge_indices, num_nodes=node_features.size(0))\n",
        "        # Compute degree and normalization factor\n",
        "        row, col = edge_indices\n",
        "        degree_vector = degree(col, node_features.size(0), dtype=node_features.dtype)\n",
        "        degree_inv_sqrt = degree_vector.pow(-0.5)\n",
        "        norm = degree_inv_sqrt[row] * degree_inv_sqrt[col]\n",
        "\n",
        "        # Propagate messages between nodes (i.e., perform graph convolution)\n",
        "        return self.propagate(edge_indices, x=node_features, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        # Scale neighbor features by normalization factor\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "    def update(self, aggregated_messages):\n",
        "        # Apply the learned linear transformation to the aggregated features\n",
        "        return self.linear_transform(aggregated_messages)\n",
        "\n",
        "\n",
        "class CustomGCN(nn.Module):\n",
        "    def __init__(self, node_input_dim, hidden_dim, output_dim):\n",
        "        super(CustomGCN, self).__init__()\n",
        "        self.gcn_layer1 = CustomGCNLayer(node_input_dim, hidden_dim)\n",
        "        self.gcn_layer2 = CustomGCNLayer(hidden_dim, hidden_dim)\n",
        "        self.gcn_layer3 = CustomGCNLayer(hidden_dim, hidden_dim)\n",
        "        self.gcn_layer4 = CustomGCNLayer(hidden_dim, hidden_dim)\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Extract node features, edge indices, and batch information from the input graph\n",
        "        node_features, edge_indices, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply the first GCN layer followed by ReLU activation\n",
        "        node_features = F.relu(self.gcn_layer1(node_features, edge_indices))\n",
        "\n",
        "\n",
        "        node_features = F.relu(self.gcn_layer2(node_features, edge_indices))\n",
        "\n",
        "\n",
        "        node_features = F.relu(self.gcn_layer3(node_features, edge_indices))\n",
        "\n",
        "        # Apply the fourth GCN layer (no activation here)\n",
        "        node_features = self.gcn_layer4(node_features, edge_indices)\n",
        "\n",
        "\n",
        "        graph_features = global_mean_pool(node_features, batch)\n",
        "\n",
        "\n",
        "        graph_output = self.output_layer(graph_features)\n",
        "\n",
        "        return graph_output\n"
      ],
      "metadata": {
        "id": "4IVIaTxLZOyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model on the training data\n",
        "def train_model(model, data_loader, optimizer, loss_function):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        predictions = model(batch)  # Forward pass\n",
        "        loss = loss_function(predictions, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()  # Accumulate the loss\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader, loss_function):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    actual_values = []\n",
        "    predicted_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            predictions = model(batch)\n",
        "            loss = loss_function(predictions, batch.y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Collect predictions and true values for RMSE calculation\n",
        "            predicted_values.append(predictions.cpu().numpy())\n",
        "            actual_values.append(batch.y.cpu().numpy())  # Move true values to CPU and store\n",
        "\n",
        "    # Flatten lists and compute RMSE\n",
        "    predicted_values = np.concatenate(predicted_values, axis=0)\n",
        "    actual_values = np.concatenate(actual_values, axis=0)  # Combine true values\n",
        "    rmse_value = mean_squared_error(actual_values, predicted_values, squared=False)\n",
        "\n",
        "    return total_loss / len(data_loader), rmse_value\n"
      ],
      "metadata": {
        "id": "t97NMCpKZe3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the GCN model, optimizer, and loss function\n",
        "model = CustomGCN(node_input_dim=dataset.num_node_features, hidden_dim=64, output_dim=19)  # Adjusted to 19 output dimensions\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_model(model, train_loader, optimizer, loss_function)\n",
        "    val_loss, val_rmse = evaluate_model(model, val_loader, loss_function)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation RMSE: {val_rmse:.4f}')\n",
        "\n",
        "# Test the model on the test set and report the RMSE\n",
        "test_loss, test_rmse = evaluate_model(model, test_loader, loss_function)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test RMSE: {test_rmse:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJys1mzQZrFI",
        "outputId": "5b121833-385c-43f0-eec5-25a9a50d5b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 32296331.9000, Validation Loss: 9597495.5000, Validation RMSE: 1531.4095\n",
            "Epoch 2, Training Loss: 21948447.0031, Validation Loss: 1999896.5625, Validation RMSE: 768.3661\n",
            "Epoch 3, Training Loss: 20631206.0453, Validation Loss: 845039.7500, Validation RMSE: 479.4162\n",
            "Epoch 4, Training Loss: 20513828.9094, Validation Loss: 405293.4375, Validation RMSE: 336.6413\n",
            "Epoch 5, Training Loss: 20490119.8977, Validation Loss: 562641.4375, Validation RMSE: 401.1791\n",
            "Epoch 6, Training Loss: 20497012.1465, Validation Loss: 486030.3594, Validation RMSE: 364.7804\n",
            "Epoch 7, Training Loss: 20488393.4898, Validation Loss: 559117.2969, Validation RMSE: 381.3387\n",
            "Epoch 8, Training Loss: 20473330.7367, Validation Loss: 513033.6250, Validation RMSE: 383.0806\n",
            "Epoch 9, Training Loss: 20493252.5781, Validation Loss: 419295.1719, Validation RMSE: 341.3011\n",
            "Epoch 10, Training Loss: 20482585.5328, Validation Loss: 528169.5156, Validation RMSE: 371.7399\n",
            "Epoch 11, Training Loss: 20472571.3422, Validation Loss: 601539.2969, Validation RMSE: 414.6663\n",
            "Epoch 12, Training Loss: 20482433.8711, Validation Loss: 475850.0469, Validation RMSE: 364.9027\n",
            "Epoch 13, Training Loss: 20499595.0020, Validation Loss: 380767.9844, Validation RMSE: 304.4379\n",
            "Epoch 14, Training Loss: 20520586.2391, Validation Loss: 681928.6562, Validation RMSE: 436.8513\n",
            "Epoch 15, Training Loss: 20480607.9043, Validation Loss: 472585.0781, Validation RMSE: 361.1577\n",
            "Epoch 16, Training Loss: 20479137.7891, Validation Loss: 423093.8516, Validation RMSE: 332.5679\n",
            "Epoch 17, Training Loss: 20466177.4758, Validation Loss: 373935.0859, Validation RMSE: 324.7725\n",
            "Epoch 18, Training Loss: 20505600.2230, Validation Loss: 644069.2031, Validation RMSE: 424.8068\n",
            "Epoch 19, Training Loss: 20473554.7344, Validation Loss: 477446.9375, Validation RMSE: 361.4519\n",
            "Epoch 20, Training Loss: 20471077.9750, Validation Loss: 437792.3438, Validation RMSE: 351.0257\n",
            "Epoch 21, Training Loss: 20471885.0234, Validation Loss: 620973.1719, Validation RMSE: 411.7333\n",
            "Epoch 22, Training Loss: 20475237.4602, Validation Loss: 510914.5625, Validation RMSE: 369.3903\n",
            "Epoch 23, Training Loss: 20483201.4891, Validation Loss: 511764.5000, Validation RMSE: 368.0370\n",
            "Epoch 24, Training Loss: 20468295.1562, Validation Loss: 689991.9375, Validation RMSE: 434.0818\n",
            "Epoch 25, Training Loss: 20468202.5703, Validation Loss: 524330.1719, Validation RMSE: 381.6923\n",
            "Epoch 26, Training Loss: 20467433.5703, Validation Loss: 415549.6172, Validation RMSE: 346.7012\n",
            "Epoch 27, Training Loss: 20463871.9438, Validation Loss: 550554.2344, Validation RMSE: 389.1292\n",
            "Epoch 28, Training Loss: 20473023.4371, Validation Loss: 408792.9922, Validation RMSE: 340.7469\n",
            "Epoch 29, Training Loss: 20478044.4605, Validation Loss: 638319.9219, Validation RMSE: 403.6458\n",
            "Epoch 30, Training Loss: 20475647.4641, Validation Loss: 716524.5938, Validation RMSE: 442.2569\n",
            "Epoch 31, Training Loss: 20470057.8211, Validation Loss: 567618.1719, Validation RMSE: 394.3236\n",
            "Epoch 32, Training Loss: 20474252.2656, Validation Loss: 309845.1328, Validation RMSE: 290.6104\n",
            "Epoch 33, Training Loss: 20484531.6766, Validation Loss: 393402.8438, Validation RMSE: 338.7440\n",
            "Epoch 34, Training Loss: 20468474.5664, Validation Loss: 581727.2812, Validation RMSE: 395.5552\n",
            "Epoch 35, Training Loss: 20474424.0578, Validation Loss: 469638.5938, Validation RMSE: 364.0880\n",
            "Epoch 36, Training Loss: 20484872.9672, Validation Loss: 329315.2266, Validation RMSE: 310.1980\n",
            "Epoch 37, Training Loss: 20467417.4484, Validation Loss: 417765.5781, Validation RMSE: 342.4507\n",
            "Epoch 38, Training Loss: 20476654.0680, Validation Loss: 304295.0547, Validation RMSE: 292.4841\n",
            "Epoch 39, Training Loss: 20477512.1480, Validation Loss: 341693.1484, Validation RMSE: 302.2013\n",
            "Epoch 40, Training Loss: 20477168.4758, Validation Loss: 292273.5781, Validation RMSE: 294.7548\n",
            "Epoch 41, Training Loss: 20492816.3895, Validation Loss: 311059.5312, Validation RMSE: 283.3643\n",
            "Epoch 42, Training Loss: 20489731.0789, Validation Loss: 478232.7344, Validation RMSE: 368.6909\n",
            "Epoch 43, Training Loss: 20468362.0156, Validation Loss: 365590.6797, Validation RMSE: 319.7408\n",
            "Epoch 44, Training Loss: 20464480.7063, Validation Loss: 513557.6875, Validation RMSE: 369.8179\n",
            "Epoch 45, Training Loss: 20471900.9332, Validation Loss: 299910.1562, Validation RMSE: 283.3616\n",
            "Epoch 46, Training Loss: 20498143.4742, Validation Loss: 296141.2344, Validation RMSE: 297.1847\n",
            "Epoch 47, Training Loss: 20488524.9398, Validation Loss: 647560.6094, Validation RMSE: 409.8404\n",
            "Epoch 48, Training Loss: 20478371.7344, Validation Loss: 441996.0000, Validation RMSE: 352.2666\n",
            "Epoch 49, Training Loss: 20471734.6023, Validation Loss: 623253.1250, Validation RMSE: 397.1855\n",
            "Epoch 50, Training Loss: 20467236.8891, Validation Loss: 341803.6797, Validation RMSE: 316.8129\n",
            "Epoch 51, Training Loss: 20479886.2574, Validation Loss: 307554.9766, Validation RMSE: 285.3870\n",
            "Epoch 52, Training Loss: 20482349.2688, Validation Loss: 627596.3594, Validation RMSE: 415.3516\n",
            "Epoch 53, Training Loss: 20468874.7945, Validation Loss: 678172.5781, Validation RMSE: 426.2028\n",
            "Epoch 54, Training Loss: 20468647.1957, Validation Loss: 625593.3438, Validation RMSE: 404.2587\n",
            "Epoch 55, Training Loss: 20481906.3164, Validation Loss: 658661.4688, Validation RMSE: 422.6583\n",
            "Epoch 56, Training Loss: 20458075.2703, Validation Loss: 548689.4219, Validation RMSE: 387.3046\n",
            "Epoch 57, Training Loss: 20478551.0355, Validation Loss: 614193.9062, Validation RMSE: 403.2771\n",
            "Epoch 58, Training Loss: 20470504.5395, Validation Loss: 504372.0000, Validation RMSE: 365.2370\n",
            "Epoch 59, Training Loss: 20461363.4332, Validation Loss: 592268.8750, Validation RMSE: 404.4354\n",
            "Epoch 60, Training Loss: 20462485.6922, Validation Loss: 479417.3281, Validation RMSE: 363.4911\n",
            "Epoch 61, Training Loss: 20474918.3938, Validation Loss: 230584.3945, Validation RMSE: 263.2752\n",
            "Epoch 62, Training Loss: 20496236.3648, Validation Loss: 536794.4844, Validation RMSE: 380.4262\n",
            "Epoch 63, Training Loss: 20471197.3613, Validation Loss: 558980.2344, Validation RMSE: 379.5885\n",
            "Epoch 64, Training Loss: 20459336.6320, Validation Loss: 727654.0000, Validation RMSE: 443.2339\n",
            "Epoch 65, Training Loss: 20471192.4039, Validation Loss: 556079.5938, Validation RMSE: 394.5566\n",
            "Epoch 66, Training Loss: 20460409.2437, Validation Loss: 248023.2461, Validation RMSE: 271.1063\n",
            "Epoch 67, Training Loss: 20492573.7086, Validation Loss: 253178.5156, Validation RMSE: 271.5728\n",
            "Epoch 68, Training Loss: 20485877.0352, Validation Loss: 548043.3125, Validation RMSE: 385.8788\n",
            "Epoch 69, Training Loss: 20482003.5555, Validation Loss: 554712.3750, Validation RMSE: 381.6386\n",
            "Epoch 70, Training Loss: 20470266.6289, Validation Loss: 341875.1016, Validation RMSE: 317.8900\n",
            "Epoch 71, Training Loss: 20464772.3984, Validation Loss: 654793.0625, Validation RMSE: 417.4871\n",
            "Epoch 72, Training Loss: 20500842.0836, Validation Loss: 209380.1836, Validation RMSE: 239.4701\n",
            "Epoch 73, Training Loss: 20485375.6570, Validation Loss: 318474.0312, Validation RMSE: 310.5235\n",
            "Epoch 74, Training Loss: 20468195.0562, Validation Loss: 396681.2578, Validation RMSE: 333.7638\n",
            "Epoch 75, Training Loss: 20483836.9563, Validation Loss: 166318.3047, Validation RMSE: 221.4188\n",
            "Epoch 76, Training Loss: 20472401.1621, Validation Loss: 499470.6875, Validation RMSE: 369.5438\n",
            "Epoch 77, Training Loss: 20466012.6105, Validation Loss: 615465.1406, Validation RMSE: 414.6584\n",
            "Epoch 78, Training Loss: 20462077.2477, Validation Loss: 542595.7344, Validation RMSE: 386.5222\n",
            "Epoch 79, Training Loss: 20469622.9652, Validation Loss: 488385.8906, Validation RMSE: 368.7932\n",
            "Epoch 80, Training Loss: 20485561.2734, Validation Loss: 267904.8594, Validation RMSE: 281.2005\n",
            "Epoch 81, Training Loss: 20497500.3430, Validation Loss: 347015.3516, Validation RMSE: 305.0795\n",
            "Epoch 82, Training Loss: 20482255.9828, Validation Loss: 623360.1406, Validation RMSE: 411.2004\n",
            "Epoch 83, Training Loss: 20473756.4156, Validation Loss: 392175.3984, Validation RMSE: 323.0310\n",
            "Epoch 84, Training Loss: 20504269.5805, Validation Loss: 423451.5000, Validation RMSE: 348.0136\n",
            "Epoch 85, Training Loss: 20464795.3352, Validation Loss: 341024.7031, Validation RMSE: 312.7495\n",
            "Epoch 86, Training Loss: 20476397.6727, Validation Loss: 461326.9531, Validation RMSE: 345.9666\n",
            "Epoch 87, Training Loss: 20491658.7852, Validation Loss: 602800.7031, Validation RMSE: 409.8332\n",
            "Epoch 88, Training Loss: 20462780.0883, Validation Loss: 552272.8438, Validation RMSE: 388.5634\n",
            "Epoch 89, Training Loss: 20463548.5703, Validation Loss: 604576.5000, Validation RMSE: 399.3891\n",
            "Epoch 90, Training Loss: 20490701.6445, Validation Loss: 310426.2500, Validation RMSE: 303.2968\n",
            "Epoch 91, Training Loss: 20463799.6543, Validation Loss: 627413.2812, Validation RMSE: 409.7279\n",
            "Epoch 92, Training Loss: 20459621.4805, Validation Loss: 497124.5156, Validation RMSE: 366.9405\n",
            "Epoch 93, Training Loss: 20472267.5652, Validation Loss: 298976.0703, Validation RMSE: 283.1602\n",
            "Epoch 94, Training Loss: 20487891.7930, Validation Loss: 851857.5000, Validation RMSE: 471.6866\n",
            "Epoch 95, Training Loss: 20499551.9937, Validation Loss: 237434.0898, Validation RMSE: 267.8430\n",
            "Epoch 96, Training Loss: 20483814.4937, Validation Loss: 289803.8203, Validation RMSE: 293.9968\n",
            "Epoch 97, Training Loss: 20481996.0422, Validation Loss: 518438.9219, Validation RMSE: 366.5354\n",
            "Epoch 98, Training Loss: 20464679.1477, Validation Loss: 635141.4844, Validation RMSE: 408.9747\n",
            "Epoch 99, Training Loss: 20491510.2836, Validation Loss: 546871.0312, Validation RMSE: 391.8954\n",
            "Epoch 100, Training Loss: 20471440.0117, Validation Loss: 420198.9609, Validation RMSE: 328.7552\n",
            "Test Loss: 464178.6562, Test RMSE: 346.6577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part-3:\n",
        "* Implement an attention-based GNN, incorporating the concepts of the\n",
        " Edge-Weighted Graph Attention Network (EGATConv)\n",
        "* Youcantry out various aggregators like ‘sum’, ‘mean’ etc to get graph\n",
        " features at the end.\n",
        "* Showtheeffect of EGATConv layers into the learning [use upto 4 layers].\n",
        "* Perform Regression on the test set and report RMSE\n"
      ],
      "metadata": {
        "id": "UiVJi1hTELGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Edge-Guided Graph Attention Layer\n",
        "class CustomEGATConv(MessagePassing):\n",
        "    def __init__(self, input_dim, output_dim, edge_dim, num_heads=1):\n",
        "        super(CustomEGATConv, self).__init__(aggr='add', node_dim=0)  # Using \"add\" aggregation\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.edge_dim = edge_dim\n",
        "\n",
        "\n",
        "        self.node_lin = nn.Linear(input_dim, num_heads * output_dim, bias=False)\n",
        "\n",
        "\n",
        "        self.attn_layer = nn.Linear(2 * output_dim + edge_dim, 1)\n",
        "\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        # Initialize weights with Xavier uniform distribution\n",
        "        nn.init.xavier_uniform_(self.node_lin.weight)\n",
        "        nn.init.xavier_uniform_(self.attn_layer.weight)\n",
        "\n",
        "    def forward(self, node_features, edge_index, edge_features):\n",
        "        # Apply linear transformation on node features\n",
        "        node_features = self.node_lin(node_features)\n",
        "        node_features = node_features.view(-1, self.num_heads, self.output_dim)\n",
        "\n",
        "        # Perform message passing with attention\n",
        "        return self.propagate(edge_index, x=node_features, edge_attr=edge_features)\n",
        "\n",
        "    def message(self, x_i, x_j, edge_attr, index, ptr, size_i):\n",
        "        # Reshape edge attributes for concatenation\n",
        "        edge_attr = edge_attr.view(-1, 1, edge_attr.size(-1))\n",
        "\n",
        "        # Reshape node features and concatenate with edge features\n",
        "        x_i = x_i.view(-1, self.num_heads, self.output_dim)\n",
        "        x_j = x_j.view(-1, self.num_heads, self.output_dim)\n",
        "        combined_features = torch.cat([x_i, x_j, edge_attr.repeat(1, self.num_heads, 1)], dim=-1)\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_scores = F.leaky_relu(self.attn_layer(combined_features).squeeze(-1), negative_slope=0.2)\n",
        "\n",
        "        attention_weights = softmax(attention_scores, index)\n",
        "\n",
        "        # Weight the neighboring node features by the attention coefficients\n",
        "        return attention_weights.unsqueeze(-1) * x_j\n",
        "\n",
        "    def update(self, aggregated_messages):\n",
        "        # Return the final aggregated output by averaging across heads\n",
        "        return aggregated_messages.mean(dim=1)\n",
        "\n",
        "# Attention-based Graph Neural Network (EGAT) model\n",
        "class CustomEGATModel(nn.Module):\n",
        "    def __init__(self, input_dim, edge_dim, hidden_dim, output_dim):\n",
        "        super(CustomEGATModel, self).__init__()\n",
        "\n",
        "        # Define multiple EGAT layers\n",
        "        self.egat_conv1 = CustomEGATConv(input_dim, hidden_dim, edge_dim, num_heads=1)\n",
        "        self.egat_conv2 = CustomEGATConv(hidden_dim, hidden_dim, edge_dim, num_heads=1)\n",
        "        self.egat_conv3 = CustomEGATConv(hidden_dim, hidden_dim, edge_dim, num_heads=1)\n",
        "        self.egat_conv4 = CustomEGATConv(hidden_dim, hidden_dim, edge_dim, num_heads=1)\n",
        "\n",
        "        # Final linear layer for regression\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Extract relevant data attributes (node features, edges, and edge features)\n",
        "        node_features, edge_index, edge_features, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "\n",
        "        node_features = F.relu(self.egat_conv1(node_features, edge_index, edge_features))\n",
        "\n",
        "        node_features = F.relu(self.egat_conv2(node_features, edge_index, edge_features))\n",
        "\n",
        "        node_features = F.relu(self.egat_conv3(node_features, edge_index, edge_features))\n",
        "\n",
        "        node_features = self.egat_conv4(node_features, edge_index, edge_features)\n",
        "\n",
        "        # Pool the node features into a graph-level representation\n",
        "        graph_representation = global_mean_pool(node_features, batch)\n",
        "\n",
        "        # Use the final linear layer for regression task\n",
        "        return self.output_layer(graph_representation)\n"
      ],
      "metadata": {
        "id": "YIgt3yN1dj75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform model training\n",
        "def train_model(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        predictions = model(batch)  # Forward pass\n",
        "        loss = criterion(predictions, batch.y)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update parameters\n",
        "        total_train_loss += loss.item()  # Accumulate loss\n",
        "    average_train_loss = total_train_loss / len(loader)  # Average loss per batch\n",
        "    return average_train_loss\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_true_values = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            predictions = model(batch)  # Forward pass\n",
        "            loss = criterion(predictions, batch.y)  # Compute loss\n",
        "            total_val_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "            # Collect predictions and true values\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_true_values.append(batch.y.cpu().numpy())\n",
        "\n",
        "    # Concatenate and compute RMSE\n",
        "    predictions = np.concatenate(all_predictions, axis=0)\n",
        "    true_values = np.concatenate(all_true_values, axis=0)\n",
        "    rmse_score = mean_squared_error(true_values, predictions, squared=False)\n",
        "    average_val_loss = total_val_loss / len(loader)  # Average loss per batch\n",
        "\n",
        "    return average_val_loss, rmse_score\n",
        "\n",
        "# Set up the model, optimizer, and loss function\n",
        "model = EGATModel(num_node_features=dataset.num_node_features,\n",
        "                  num_edge_features=dataset.num_edge_features,\n",
        "                  hidden_channels=64,\n",
        "                  num_classes=19)  # Number of output classes set to 19\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer\n",
        "loss_function = nn.MSELoss()  # Loss function for regression\n",
        "\n",
        "# Training and evaluation loop\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    training_loss = train_model(model, train_loader, optimizer, loss_function)\n",
        "    validation_loss, validation_rmse = evaluate_model(model, val_loader, loss_function)\n",
        "    print(f'Epoch {epoch + 1}: Training Loss: {training_loss:.4f}, Validation Loss: {validation_loss:.4f}, Validation RMSE: {validation_rmse:.4f}')\n",
        "\n",
        "# Evaluate model on the test set\n",
        "test_loss, test_rmse = evaluate_model(model, test_loader, loss_function)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test RMSE: {test_rmse:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl3t702EftmF",
        "outputId": "76657e51-3139-40ab-cfbe-d6f8f40a6d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Training Loss: 30752615.8938, Validation Loss: 1056368.5312, Validation RMSE: 605.3462\n",
            "Epoch 2: Training Loss: 21317321.8086, Validation Loss: 280282.3672, Validation RMSE: 308.0165\n",
            "Epoch 3: Training Loss: 20614601.8250, Validation Loss: 565800.4375, Validation RMSE: 396.3531\n",
            "Epoch 4: Training Loss: 20478109.0000, Validation Loss: 641567.4844, Validation RMSE: 391.8039\n",
            "Epoch 5: Training Loss: 20487089.6812, Validation Loss: 479095.2891, Validation RMSE: 353.4192\n",
            "Epoch 6: Training Loss: 20489859.2598, Validation Loss: 633021.0469, Validation RMSE: 412.3197\n",
            "Epoch 7: Training Loss: 20476869.2109, Validation Loss: 624366.4531, Validation RMSE: 409.4086\n",
            "Epoch 8: Training Loss: 20459560.7820, Validation Loss: 464726.2109, Validation RMSE: 336.2867\n",
            "Epoch 9: Training Loss: 20478980.6766, Validation Loss: 521926.1953, Validation RMSE: 353.6171\n",
            "Epoch 10: Training Loss: 20533319.3031, Validation Loss: 522738.7734, Validation RMSE: 378.8055\n",
            "Epoch 11: Training Loss: 20484002.2113, Validation Loss: 596584.9062, Validation RMSE: 390.1585\n",
            "Epoch 12: Training Loss: 20490332.6812, Validation Loss: 653960.6094, Validation RMSE: 410.2472\n",
            "Epoch 13: Training Loss: 20476815.6672, Validation Loss: 773658.9844, Validation RMSE: 430.8237\n",
            "Epoch 14: Training Loss: 20511351.4898, Validation Loss: 541631.4531, Validation RMSE: 383.5245\n",
            "Epoch 15: Training Loss: 20471996.7035, Validation Loss: 505115.6094, Validation RMSE: 365.0173\n",
            "Epoch 16: Training Loss: 20456636.7945, Validation Loss: 463474.4609, Validation RMSE: 354.9293\n",
            "Epoch 17: Training Loss: 20451589.9523, Validation Loss: 600599.8203, Validation RMSE: 397.9188\n",
            "Epoch 18: Training Loss: 20439050.8383, Validation Loss: 515439.3203, Validation RMSE: 362.8010\n",
            "Epoch 19: Training Loss: 20447233.8188, Validation Loss: 449093.8008, Validation RMSE: 348.8657\n",
            "Epoch 20: Training Loss: 20432522.1195, Validation Loss: 530188.5781, Validation RMSE: 385.1585\n",
            "Test Loss: 650747.1562, Test RMSE: 425.0765\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}